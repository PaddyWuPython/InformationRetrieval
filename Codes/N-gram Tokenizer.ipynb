{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC overview of the results of a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_at_1 (relevant, retrieved):\n",
    "    if len(retrieved) > 0 and retrieved[0] in relevant:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def success_at_5(relevant, retrieved):\n",
    "\n",
    "    for item in range(0, 5):\n",
    "        if retrieved[item] in relevant:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def success_at_10(relevant, retrieved):\n",
    "\n",
    "    for item in range(0, 10):\n",
    "        if retrieved[item] in relevant:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def precision_at_k(relevant, retrieved, k):\n",
    "\n",
    "    relevant_at_k = []\n",
    "    for doc in retrieved[:k]:\n",
    "        if doc in relevant:\n",
    "            relevant_at_k.append(doc)\n",
    "    return len(relevant_at_k) / k\n",
    "\n",
    "def r_precision(relevant, retrieved):\n",
    "\n",
    "    R = len(relevant)\n",
    "    relevant_at_R = []\n",
    "    for doc in retrieved[:R]:\n",
    "        if doc in relevant:\n",
    "            relevant_at_R.append(doc)\n",
    "    if R>0:\n",
    "        return len(relevant_at_R) / R\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def average_precision(relevant, retrieved):\n",
    "\n",
    "    relevant_retrieved = 0\n",
    "    precision_sum = 0.0\n",
    "    \n",
    "    for i, doc in enumerate(retrieved):\n",
    "        if doc in relevant:\n",
    "            relevant_retrieved += 1\n",
    "            precision_sum += relevant_retrieved / (i + 1)\n",
    "    \n",
    "    if relevant_retrieved == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return precision_sum / relevant_retrieved\n",
    "    \n",
    "def interpolated_precision_at_recall_X (relevant, retrieved, X):\n",
    "\n",
    "    relevant_retrieved = 0\n",
    "    precision_at_recall = []\n",
    "\n",
    "    for i, doc in enumerate(retrieved):\n",
    "        if doc in relevant:\n",
    "            relevant_retrieved += 1\n",
    "        recall = relevant_retrieved / len(relevant)\n",
    "        precision = relevant_retrieved / (i + 1)\n",
    "        if recall >= X:\n",
    "            precision_at_recall.append(precision)\n",
    "    if precision_at_recall:\n",
    "        return max(precision_at_recall)\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def mean_metric(measure, all_relevant, all_retrieved):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for qid in all_relevant:\n",
    "        relevant  = all_relevant[qid]\n",
    "        retrieved = all_retrieved.get(qid, [])\n",
    "        value = measure(relevant, retrieved)\n",
    "        total += value\n",
    "        count += 1\n",
    "    return \"mean \" + measure.__name__, total / count\n",
    "\n",
    "\n",
    "def trec_eval(qrels_file, run_file):\n",
    "\n",
    "    def precision_at_1(rel, ret): return precision_at_k(rel, ret, k=1)\n",
    "    def precision_at_5(rel, ret): return precision_at_k(rel, ret, k=5)\n",
    "    def precision_at_10(rel, ret): return precision_at_k(rel, ret, k=10)\n",
    "    def precision_at_50(rel, ret): return precision_at_k(rel, ret, k=50)\n",
    "    def precision_at_100(rel, ret): return precision_at_k(rel, ret, k=100)\n",
    "    def precision_at_recall_00(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.0)\n",
    "    def precision_at_recall_01(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.1)\n",
    "    def precision_at_recall_02(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.2)\n",
    "    def precision_at_recall_03(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.3)\n",
    "    def precision_at_recall_04(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.4)\n",
    "    def precision_at_recall_05(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.5)\n",
    "    def precision_at_recall_06(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.6)\n",
    "    def precision_at_recall_07(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.7)\n",
    "    def precision_at_recall_08(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.8)\n",
    "    def precision_at_recall_09(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.9)\n",
    "    def precision_at_recall_10(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=1.0)\n",
    "\n",
    "    (all_relevant, all_retrieved) = read_eval_files(qrels_file, run_file)\n",
    "    \n",
    "    unknown_qids = set(all_retrieved.keys()).difference(all_relevant.keys())\n",
    "    if len(unknown_qids) > 0:\n",
    "        raise ValueError(\"Unknown qids in run: {}\".format(sorted(list(unknown_qids))))\n",
    "\n",
    "    metrics = [success_at_1,\n",
    "               success_at_5,\n",
    "               success_at_10,\n",
    "               r_precision,\n",
    "               precision_at_1,\n",
    "               precision_at_5,\n",
    "               precision_at_10,\n",
    "               precision_at_50,\n",
    "               precision_at_100,\n",
    "               precision_at_recall_00,\n",
    "               precision_at_recall_01,\n",
    "               precision_at_recall_02,\n",
    "               precision_at_recall_03,\n",
    "               precision_at_recall_04,\n",
    "               precision_at_recall_05,\n",
    "               precision_at_recall_06,\n",
    "               precision_at_recall_07,\n",
    "               precision_at_recall_08,\n",
    "               precision_at_recall_09,\n",
    "               precision_at_recall_10,\n",
    "               average_precision]\n",
    "\n",
    "    return [mean_metric(metric, all_relevant, all_retrieved) for metric in metrics]\n",
    "\n",
    "\n",
    "def print_trec_eval(qrels_file, run_file):\n",
    "    results = trec_eval(qrels_file, run_file)\n",
    "    print(\"Results for {}\".format(run_file))\n",
    "    for (metric, score) in results:\n",
    "        print(\"{:<30} {:.4}\".format(metric, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_qrels_file(qrels_file):  # reads the content of he qrels file\n",
    "    trec_relevant = dict()  # query_id -> set([docid1, docid2, ...])\n",
    "    with open(qrels_file, 'r') as qrels:\n",
    "        for line in qrels:\n",
    "            (qid, q0, doc_id, rel) = line.strip().split()\n",
    "            if qid not in trec_relevant:\n",
    "                trec_relevant[qid] = set()\n",
    "            if (rel == \"1\"):\n",
    "                trec_relevant[qid].add(doc_id)\n",
    "    return trec_relevant\n",
    "\n",
    "def read_run_file(run_file):  \n",
    "    # read the content of the run file produced by our IR system \n",
    "    # (in the following exercises you will create your own run_files)\n",
    "    trec_retrieved = dict()  # query_id -> [docid1, docid2, ...]\n",
    "    with open(run_file, 'r') as run:\n",
    "        for line in run:\n",
    "            (qid, q0, doc_id, rank, score, tag) = line.strip().split()\n",
    "            if qid not in trec_retrieved:\n",
    "                trec_retrieved[qid] = []\n",
    "            trec_retrieved[qid].append(doc_id) \n",
    "    return trec_retrieved\n",
    "    \n",
    "\n",
    "def read_eval_files(qrels_file, run_file):\n",
    "    return read_qrels_file(qrels_file), read_run_file(run_file)\n",
    "\n",
    "(all_relevant, all_retrieved) = read_eval_files('data01/FIR-s05-training-qrels.txt', 'data01/baseline.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "import elasticsearch.helpers\n",
    "import json\n",
    "\n",
    "def read_documents(file_name):\n",
    "    \"\"\"\n",
    "    Returns a generator of documents to be indexed by elastic, read from file_name\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r') as documents:\n",
    "        for line in documents:\n",
    "            doc_line = json.loads(line)\n",
    "            if ('index' in doc_line):\n",
    "                id = doc_line['index']['_id']\n",
    "            elif ('PMID' in doc_line):\n",
    "                doc_line['_id'] = id\n",
    "                yield doc_line\n",
    "            else:\n",
    "                raise ValueError('Woops, error in index file')\n",
    "\n",
    "def create_index(es, index_name, body={}):\n",
    "    # delete index when it already exists\n",
    "    es.indices.delete(index=index_name, ignore=[400, 404])\n",
    "    # create the index \n",
    "    es.indices.create(index=index_name, body=body)\n",
    "                \n",
    "def index_documents(es, collection_file_name, index_name, body={}):\n",
    "    create_index(es, index_name, body)\n",
    "    # bulk index the documents from file_name\n",
    "    return elasticsearch.helpers.bulk(\n",
    "        es, \n",
    "        read_documents(collection_file_name),\n",
    "        index=index_name,\n",
    "        chunk_size=2000,\n",
    "        request_timeout=30\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making your own TREC run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "def make_trec_run(es, topics_file_name, run_file_name, index_name=\"genomics\", run_name=\"test\"):\n",
    "    with open(run_file_name, 'w') as run_file:\n",
    "        with open(topics_file_name, 'r') as test_queries:\n",
    "            for line in test_queries:\n",
    "                (qid, query) = line.strip().split('\\t')\n",
    "                # BEGIN ANSWER\n",
    "                body = {\n",
    "                    \"query\":{\n",
    "                        \"multi_match\":{\n",
    "                            \"query\":query,\n",
    "                            \"fields\":[\"TI\"]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                response = es.search(index = index_name, body = body,size =1000)\n",
    "                                    \n",
    "                rank = 0\n",
    "                for hit in response['hits']['hits']:\n",
    "                    pmid = hit['_source']['PMID']\n",
    "                    score = hit['_score']\n",
    "\n",
    "                    run_file.write(\"{} Q0 {} {} {} {}\\n\".format(str(qid), str(pmid), str(rank), str(score), run_name))\n",
    "                    rank+=1\n",
    "                # END ANSWER\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jelinek-Mercer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS GRADED!\n",
    "\n",
    "lmjelinekmercer = {\n",
    "  # BEGIN ANSWER\n",
    "    \"settings\": {\n",
    "        \"index\":{\n",
    "            \"similarity\":{\n",
    "                \"LMJ_similarity\":{\n",
    "                    \"type\":\"LMJelinekMercer\",\n",
    "                    \"lambda\":0.2\n",
    "                }\n",
    "\n",
    "            },\n",
    "            \"number_of_shards\": 1\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"TI\": {\n",
    "                \"type\": \"text\",\n",
    "                \"copy_to\": \"all\",\n",
    "                \"similarity\": \"LMJ_similarity\"\n",
    "            },\n",
    "            \"AB\": {\n",
    "                \"type\": \"text\",\n",
    "                \"copy_to\": \"all\",\n",
    "                \"similarity\": \"LMJ_similarity\"\n",
    "            },\n",
    "            \"all\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"LMJ_similarity\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "  # END ANSWER\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/elasticsearch/connection/base.py:190: ElasticsearchDeprecationWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.14/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "es = elasticsearch.Elasticsearch('localhost',timeout=60)\n",
    "\n",
    "index_documents(es, 'data01/FIR-s05-medline.json', 'genomics-jm', body=lmjelinekmercer)\n",
    "make_trec_run(es, 'data01/FIR-s05-training-queries-simple.txt', 'lmjelinekmercer.run', 'genomics-jm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Tokenizer with Jelinek-Mercer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = {\n",
    "    \"settings\": {\n",
    "    \"index\": {\n",
    "        \"max_ngram_diff\": 10,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"similarity\": {\n",
    "            \"LMJ_similarity\": {\n",
    "                \"type\": \"LMJelinekMercer\",\n",
    "                \"lambda\": 0.2\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"analysis\": {\n",
    "        \"tokenizer\": {\n",
    "            \"ngram_tokenizer\": {\n",
    "                \"type\": \"ngram\",\n",
    "                \"min_gram\": 3,\n",
    "                \"max_gram\": 10,\n",
    "                \"token_chars\": [\"letter\", \"digit\"]\n",
    "            }\n",
    "        },\n",
    "      \"analyzer\": {\n",
    "          \"ngram_analyzer\": {\n",
    "              \"type\": \"custom\",\n",
    "              \"tokenizer\": \"ngram_tokenizer\",\n",
    "              \"filter\": [\"lowercase\"]\n",
    "          }\n",
    "      }\n",
    "    }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"TI\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"ngram_analyzer\",\n",
    "                \"similarity\": \"LMJ_similarity\",\n",
    "                \"copy_to\": \"all\"\n",
    "            },\n",
    "            \"AB\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"ngram_analyzer\",\n",
    "                \"similarity\": \"LMJ_similarity\",\n",
    "                \"copy_to\": \"all\"\n",
    "            },\n",
    "            \"all\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"LMJ_similarity\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = elasticsearch.Elasticsearch('localhost',timeout=60)\n",
    "\n",
    "index_documents(es, 'data01/FIR-s05-medline.json', 'genomics_ngram', body = ngram)\n",
    "make_trec_run(es, 'data01/FIR-s05-training-queries-simple.txt', 'ngram.run', \"genomics_ngram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 score with lmjelinekmercer computed by Elasticsearch: 14.918538\n"
     ]
    }
   ],
   "source": [
    "body = {\n",
    "  \"query\": {\n",
    "    \"match\" : { \"TI\" : \"structure refinement\" }\n",
    "  }\n",
    "}\n",
    "explain = es.explain(index=\"genomics-jm\", id=\"3\", body=body)\n",
    "print (\"BM25 score with lmjelinekmercer computed by Elasticsearch:\",explain['explanation']['value'])  # BM25 score computed by ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 score with ngram_analyzer computed by Elasticsearch: 385.84772\n"
     ]
    }
   ],
   "source": [
    "body = {\n",
    "  \"query\": {\n",
    "    \"match\" : { \"TI\" : \"structure refinement\" }\n",
    "  }\n",
    "}\n",
    "\n",
    "explain = es.explain(index=\"genomics_ngram\", id=\"3\", body=body)\n",
    "print(\"BM25 score with ngram_analyzer computed by Elasticsearch:\", explain['explanation']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for genomics-jm tokenization:\n",
      "Doc ID: 36131, BM25 Score: 16.557785\n",
      "Doc ID: 36137, BM25 Score: 16.500984\n",
      "Doc ID: 144161, BM25 Score: 15.938251\n",
      "Doc ID: 488190, BM25 Score: 15.809368\n",
      "Doc ID: 48538, BM25 Score: 15.688313\n",
      "Doc ID: 219434, BM25 Score: 15.466249\n",
      "Doc ID: 3, BM25 Score: 14.918538\n",
      "Doc ID: 289779, BM25 Score: 14.692461\n",
      "Doc ID: 198386, BM25 Score: 11.182945\n",
      "Doc ID: 130401, BM25 Score: 10.846478\n"
     ]
    }
   ],
   "source": [
    "def perform_query_on_base(es, query_text):\n",
    "    \"\"\"\n",
    "    Perform a search on the genomics-base index and return the BM25 score and retrieved documents\n",
    "    \"\"\"\n",
    "    search_body = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"TI\": query_text\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = es.search(index=\"genomics-jm\", body=search_body)\n",
    "    return response\n",
    "\n",
    "result_base = perform_query_on_base(es, \"structure refinement\")\n",
    "\n",
    "scores_base = [(hit['_id'], hit['_score']) for hit in result_base['hits']['hits']]\n",
    "\n",
    "print(\"\\nResults for genomics-jm tokenization:\")\n",
    "for doc_id, score in scores_base:\n",
    "    print(f\"Doc ID: {doc_id}, BM25 Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for genomics_ngram tokenization:\n",
      "Doc ID: 48538, BM25 Score: 441.34824\n",
      "Doc ID: 36137, BM25 Score: 421.822\n",
      "Doc ID: 144161, BM25 Score: 415.12033\n",
      "Doc ID: 488190, BM25 Score: 410.2574\n",
      "Doc ID: 36131, BM25 Score: 408.253\n",
      "Doc ID: 219434, BM25 Score: 402.56943\n",
      "Doc ID: 171805, BM25 Score: 401.3041\n",
      "Doc ID: 182229, BM25 Score: 398.067\n",
      "Doc ID: 31435, BM25 Score: 396.339\n",
      "Doc ID: 404615, BM25 Score: 389.55188\n"
     ]
    }
   ],
   "source": [
    "def perform_query_on_base(es, query_text):\n",
    "    \"\"\"\n",
    "    Perform a search on the genomics-base index and return the BM25 score and retrieved documents\n",
    "    \"\"\"\n",
    "    search_body = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"TI\": query_text\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = es.search(index=\"genomics_ngram\", body=search_body)\n",
    "    return response\n",
    "\n",
    "result_base = perform_query_on_base(es, \"structure refinement\")\n",
    "\n",
    "scores_base = [(hit['_id'], hit['_score']) for hit in result_base['hits']['hits']]\n",
    "\n",
    "print(\"\\nResults for genomics_ngram tokenization:\")\n",
    "for doc_id, score in scores_base:\n",
    "    print(f\"Doc ID: {doc_id}, BM25 Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ngram.run\n",
      "mean success_at_1              0.05263\n",
      "mean success_at_5              0.1579\n",
      "mean success_at_10             0.2105\n",
      "mean r_precision               0.0335\n",
      "mean precision_at_1            0.05263\n",
      "mean precision_at_5            0.03684\n",
      "mean precision_at_10           0.02895\n",
      "mean precision_at_50           0.01474\n",
      "mean precision_at_100          0.008421\n",
      "mean precision_at_recall_00    0.1029\n",
      "mean precision_at_recall_01    0.0996\n",
      "mean precision_at_recall_02    0.09241\n",
      "mean precision_at_recall_03    0.08849\n",
      "mean precision_at_recall_04    0.06121\n",
      "mean precision_at_recall_05    0.05289\n",
      "mean precision_at_recall_06    0.03606\n",
      "mean precision_at_recall_07    0.03212\n",
      "mean precision_at_recall_08    0.02542\n",
      "mean precision_at_recall_09    0.02531\n",
      "mean precision_at_recall_10    0.02531\n",
      "mean average_precision         0.07881\n",
      "\n",
      "\n",
      "Results for lmjelinekmercer.run\n",
      "mean success_at_1              0.1579\n",
      "mean success_at_5              0.1842\n",
      "mean success_at_10             0.2368\n",
      "mean r_precision               0.1092\n",
      "mean precision_at_1            0.1579\n",
      "mean precision_at_5            0.06316\n",
      "mean precision_at_10           0.04474\n",
      "mean precision_at_50           0.01895\n",
      "mean precision_at_100          0.01211\n",
      "mean precision_at_recall_00    0.1994\n",
      "mean precision_at_recall_01    0.1994\n",
      "mean precision_at_recall_02    0.1678\n",
      "mean precision_at_recall_03    0.1618\n",
      "mean precision_at_recall_04    0.1541\n",
      "mean precision_at_recall_05    0.1508\n",
      "mean precision_at_recall_06    0.09714\n",
      "mean precision_at_recall_07    0.09435\n",
      "mean precision_at_recall_08    0.0665\n",
      "mean precision_at_recall_09    0.06631\n",
      "mean precision_at_recall_10    0.06631\n",
      "mean average_precision         0.1526\n"
     ]
    }
   ],
   "source": [
    "print_trec_eval('data01/FIR-s05-training-qrels.txt', 'ngram.run')\n",
    "print(\"\\n\")\n",
    "print_trec_eval('data01/FIR-s05-training-qrels.txt', 'lmjelinekmercer.run')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
